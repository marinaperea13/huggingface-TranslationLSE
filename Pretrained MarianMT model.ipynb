{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the metric we need to use for evaluation, in this case, SACREBLEU, and we load the dataset using datasets library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings \n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from datasets import load_dataset, load_metric\n",
    "bleu_metric = load_metric(\"sacrebleu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-6d84514352d255a9\n",
      "Reusing dataset json (/home/marina/.cache/huggingface/datasets/json/default-6d84514352d255a9/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fd47112e74f44ac89efe7e6b8af36cfa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dataset = load_dataset('json', data_files={'train': 'PHOENIX/dataset_train.json','test': 'PHOENIX/dataset_test.json'}, field=\"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'le': 'lluvia y nieve en los Alpes en la noche después en el norte y noreste, caen aguaceros aquí y allá, de lo contrario, eso está despejado',\n",
       "  'ls': 'LLUVIA REGION DE NIEVE DESAPARECIENDO LA LLUVIA DEL NORTE PUEDE VER ESTRELLAS DE REGION'}}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[\"test\"][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the following function we show some random examples of the dataset to see how data looks like."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>translation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>{'le': 'esta noche diecisiete grados en el mar báltico y cinco grados en los alpes', 'ls': 'NOCHE DIECISIETE NORTE CINCO MONTAÑA'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>{'le': 'y de la noche al viernes, las tormentas eléctricas probablemente también pueden convertirse en tormentas', 'ls': 'LOS VIERNES LAS TORMENTAS PUEDEN CONOCER EL VIENTO'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>{'le': 'En el este y sureste, aparte de los campos brumosos, todavía es en su mayoría amigable', 'ls': 'ESTE SUDESTE LUEGO LENTAMENTE SOL POSS-SER TENGA UN POCO DE NIEBLA'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>{'le': 'Es posible que haya ráfagas de viento cerca de las tormentas; de lo contrario, el viento sopla de débil a moderado en el mar del norte y también fresco en el oeste.', 'ls': 'TORMENTA POSIBLE IX TORMENTA IX DE OTRO MODO PESO PESO VIENTO IX'}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>{'le': 'esta noche en todas partes menos grados donde despeja hay heladas severas', 'ls': 'HOY INCLUSO TODO MENOS DONDE EL CIELO CLARO ANTERIORMENTE HELADA PRIORIDADES'}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import datasets\n",
    "import random\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "def show_random_elements(dataset, num_examples=5):\n",
    "    assert num_examples <= len(dataset), \"Can't pick more elements than there are in the dataset.\"\n",
    "    picks = []\n",
    "    for _ in range(num_examples):\n",
    "        pick = random.randint(0, len(dataset)-1)\n",
    "        while pick in picks:\n",
    "            pick = random.randint(0, len(dataset)-1)\n",
    "        picks.append(pick)\n",
    "    \n",
    "    df = pd.DataFrame(dataset[picks])\n",
    "    for column, typ in dataset.features.items():\n",
    "        if isinstance(typ, datasets.ClassLabel):\n",
    "            df[column] = df[column].transform(lambda i: typ.names[i])\n",
    "    display(HTML(df.to_html()))\n",
    "show_random_elements(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the pre-trained tokenizer that will tokenize the inputs and put them in a format that the model expects, and generate the other inputs that the model needs. By instantiating the tokenizer with from_pretrained function we ensure:\n",
    "- We get a tokenizer that corresponds to the architecture of the model we want to train\n",
    "- We download the vocabulary used when pretraining this specific checkpoint.\n",
    "\n",
    "We use a [pre-trained model](https://huggingface.co/transformers/v3.3.1/pretrained_models.html) for translation, in this case the [MarianMT model](https://huggingface.co/models?language=es&pipeline_tag=translation&sort=downloads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, MarianTokenizer\n",
    "\n",
    "model_marian = \"Helsinki-NLP/opus-mt-es-es\"\n",
    "tokenizer = MarianTokenizer.from_pretrained(model_marian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before feed the data to our model, we write the function that will preprocess our samples. With the argument `truncation=True` we ensure that an input longer than what the model selected can handle will be truncated to the maximum length accepted by the model. The padding will be dealt with later on (in a data collator) so we pad examples to the longest length in the batch and not the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_input_length = 128\n",
    "max_target_length = 128\n",
    "source_lang = \"ls\"\n",
    "target_lang = \"le\"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [ex[source_lang] for ex in examples[\"translation\"]]\n",
    "    targets = [ex[target_lang] for ex in examples[\"translation\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=max_input_length, truncation=True)\n",
    "\n",
    "    # Setup the tokenizer for targets\n",
    "    with tokenizer.as_target_tokenizer():\n",
    "        labels = tokenizer(targets, max_length=max_target_length, truncation=True)\n",
    "\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[58, 25801, 22021, 30206, 28989, 13780, 8641, 500, 714, 748, 7855, 10717, 0], [1516, 10314, 533, 8436, 1627, 11467, 6178, 128, 65, 1627, 11467, 6178, 128, 29970, 3467, 415, 533, 2535, 5094, 24774, 25884, 0]], 'attention_mask': [[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1], [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]], 'labels': [[18, 100, 10, 1496, 12890, 2817, 2081, 9239, 29, 223, 3829, 1204, 7, 8123, 0], [14, 897, 3199, 6430, 44, 18, 44, 22370, 365, 3, 33, 30268, 8789, 123, 9285, 6330, 0]]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess_function(dataset['train'][:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the map method of our dataset object created earlier to apply this function on all pairs of our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading cached processed dataset at /home/marina/.cache/huggingface/datasets/json/default-6d84514352d255a9/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-7a1c3109455d186a.arrow\n",
      "Loading cached processed dataset at /home/marina/.cache/huggingface/datasets/json/default-6d84514352d255a9/0.0.0/d75ead8d5cfcbe67495df0f89bd262f0023257fbbbd94a730313295f3d756d50/cache-0da66b7009c774cf.arrow\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine-tuning the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\n",
    "\n",
    "def model_init():\n",
    "    return AutoModelForSeq2SeqLM.from_pretrained(model_marian)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_marian)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a function for compute the metrics from the predictions. This function use the metric we loaded earlier and decode the predictions into texts. In addition, these decoded predictions are transcribed into a file that will be stored in the indicated directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "\n",
    "def postprocess_text(preds, labels):\n",
    "    preds = [pred.strip() for pred in preds]\n",
    "    labels = [[label.strip()] for label in labels]\n",
    "    \n",
    "    return preds, labels\n",
    "    \n",
    "def compute_metrics_bleu(eval_preds):\n",
    "    preds, labels = eval_preds\n",
    "    \n",
    "    if isinstance(preds, tuple):\n",
    "        preds = preds[0]\n",
    "        \n",
    "    decoded_preds = tokenizer.batch_decode(preds, skip_special_tokens=True)\n",
    "    decoded_labels = tokenizer.batch_decode(labels, skip_special_tokens=True)\n",
    "    \n",
    "    decoded_preds, decoded_labels = postprocess_text(decoded_preds, decoded_labels)\n",
    "    \n",
    "    result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    file = 'results-' + str(result['score']) + '.txt'\n",
    "    \n",
    "    result = {\"bleu\": result[\"score\"]}\n",
    "    \n",
    "    with open('prueba_hiperparametros/' + file, 'w') as f:\n",
    "        f.write('\\n'.join(decoded_preds))\n",
    "        \n",
    "    prediction_lens = [np.count_nonzero(pred != tokenizer.pad_token_id) for pred in preds]\n",
    "    result[\"gen_len\"] = np.mean(prediction_lens)\n",
    "    result = {k: round(v,4) for k, v in result.items()}\n",
    "    print(result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the training arguments to customize the training. It requires one folder name which will be used to save the checkpoints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Seq2SeqTrainingArguments(\n",
    "    \"prueba_hiperparametros\",\n",
    "    save_total_limit=3,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps=1,\n",
    "    fp16=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForSeq2Seq\n",
    "\n",
    "data_collator = DataCollatorForSeq2Seq(tokenizer, model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before training, we will perform the `hyperparameter_search` method returns a `BestRun` objects, which contains the value of the objective maximized (by default the sum of all metrics) and the hyperparameters it used for that run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_hp_space(trial):\n",
    "    return{\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1e-2, log=True),\n",
    "        \"num_train_epochs\": trial.suggest_int(\"num_train_epochs\", 1, 10),\n",
    "        \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [4, 8]),\n",
    "        \"weight_decay\":  trial.suggest_float(\"weight_decay\", 1e-6, 1e-1)\n",
    "\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/config.json from cache at /home/marina/.cache/huggingface/transformers/5f8704e1d92551880c1978068cafdf6503c7a82b65e4494be411d26cb7e86ae5.54793584623797a1e55aa65ed13d20fb17e92502019985cae46974750ad85593\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      33252\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 33252,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 33252,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 33253\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/pytorch_model.bin from cache at /home/marina/.cache/huggingface/transformers/415af22dd09609c5b17fafdc415111b246d167424a6cbf434e09c258728c63b6.eb1842c6d9b4f1ed8611496ed600416387a7972ee684375e1c4640f1691d7e0d\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-es-es.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "Using amp fp16 backend\n",
      "\u001b[32m[I 2022-02-17 13:18:16,355]\u001b[0m A new study created in memory with name: no-name-d952c5c6-a8f9-4cfd-85b2-2fa7d9774e12\u001b[0m\n",
      "Trial:\n",
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/config.json from cache at /home/marina/.cache/huggingface/transformers/5f8704e1d92551880c1978068cafdf6503c7a82b65e4494be411d26cb7e86ae5.54793584623797a1e55aa65ed13d20fb17e92502019985cae46974750ad85593\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      33252\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 33252,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 33252,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 33253\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/pytorch_model.bin from cache at /home/marina/.cache/huggingface/transformers/415af22dd09609c5b17fafdc415111b246d167424a6cbf434e09c258728c63b6.eb1842c6d9b4f1ed8611496ed600416387a7972ee684375e1c4640f1691d7e0d\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-es-es.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 7096\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14192' max='14192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14192/14192 13:55, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>4.725800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>4.718000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>4.914200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.366300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.359000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>5.380700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>5.433800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>5.304200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>5.367200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.424300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>5.388900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>5.406200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>5.387700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>5.363700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>5.372300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8000</td>\n",
       "      <td>5.378000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8500</td>\n",
       "      <td>5.336700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9000</td>\n",
       "      <td>5.353000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9500</td>\n",
       "      <td>5.337600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10000</td>\n",
       "      <td>5.325300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10500</td>\n",
       "      <td>5.338200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11000</td>\n",
       "      <td>5.313000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11500</td>\n",
       "      <td>5.296400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12000</td>\n",
       "      <td>5.302400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12500</td>\n",
       "      <td>5.297400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13000</td>\n",
       "      <td>5.294500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13500</td>\n",
       "      <td>5.290800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14000</td>\n",
       "      <td>5.275400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/marina/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-1000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-1000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-1000/special_tokens_map.json\n",
      "/home/marina/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-1500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-1500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-1500/special_tokens_map.json\n",
      "/home/marina/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-2000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-2000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-2500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-2500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-3000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-3000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-3500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-3500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-2000] due to args.save_total_limit\n",
      "/home/marina/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-4000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-4000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-2500] due to args.save_total_limit\n",
      "/home/marina/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-4500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-4500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-5000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-5000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-3500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-5500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-5500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-6000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-6000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-4500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-6500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-6500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-7000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-7000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-5500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-7500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-7500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-8000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-8000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-6500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-8500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-8500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-7000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-9000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-9000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-9500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-9500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-10000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-10000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-10500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-10500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-9000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-11000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-11000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-11500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-11500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-12000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-12000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-10500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-12500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-12500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-13000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-13000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-13500\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-13500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-12000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-0/checkpoint-14000\n",
      "Configuration saved in prueba_hiperparametros/run-0/checkpoint-14000/config.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model weights saved in prueba_hiperparametros/run-0/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-0/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-0/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-0/checkpoint-12500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 642\n",
      "  Batch size = 8\n",
      "/home/marina/anaconda3/lib/python3.8/site-packages/torch/_tensor.py:575: UserWarning: floor_divide is deprecated, and will be removed in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values.\n",
      "To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor'). (Triggered internally at  /pytorch/aten/src/ATen/native/BinaryOps.cpp:467.)\n",
      "  return torch.floor_divide(self, other)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='405' max='81' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [81/81 37:43]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-02-17 13:39:56,101]\u001b[0m Trial 0 finished with value: 511.0005 and parameters: {'learning_rate': 0.0007469165006449348, 'num_train_epochs': 8, 'per_device_train_batch_size': 4, 'weight_decay': 0.08783717437402438}. Best is trial 0 with value: 511.0005.\u001b[0m\n",
      "Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0005, 'gen_len': 511.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/config.json from cache at /home/marina/.cache/huggingface/transformers/5f8704e1d92551880c1978068cafdf6503c7a82b65e4494be411d26cb7e86ae5.54793584623797a1e55aa65ed13d20fb17e92502019985cae46974750ad85593\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      33252\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 33252,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 33252,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 33253\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/pytorch_model.bin from cache at /home/marina/.cache/huggingface/transformers/415af22dd09609c5b17fafdc415111b246d167424a6cbf434e09c258728c63b6.eb1842c6d9b4f1ed8611496ed600416387a7972ee684375e1c4640f1691d7e0d\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-es-es.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 7096\n",
      "  Num Epochs = 2\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3548\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3548' max='3548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3548/3548 03:30, Epoch 2/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.929200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>3.129600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.710900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.497600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.265600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.272500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>2.184800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to prueba_hiperparametros/run-1/checkpoint-500\n",
      "Configuration saved in prueba_hiperparametros/run-1/checkpoint-500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-1/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-1/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-1/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to prueba_hiperparametros/run-1/checkpoint-1000\n",
      "Configuration saved in prueba_hiperparametros/run-1/checkpoint-1000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-1/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-1/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-1/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to prueba_hiperparametros/run-1/checkpoint-1500\n",
      "Configuration saved in prueba_hiperparametros/run-1/checkpoint-1500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-1/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-1/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-1/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to prueba_hiperparametros/run-1/checkpoint-2000\n",
      "Configuration saved in prueba_hiperparametros/run-1/checkpoint-2000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-1/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-1/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-1/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-1/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-1/checkpoint-2500\n",
      "Configuration saved in prueba_hiperparametros/run-1/checkpoint-2500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-1/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-1/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-1/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-1/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-1/checkpoint-3000\n",
      "Configuration saved in prueba_hiperparametros/run-1/checkpoint-3000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-1/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-1/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-1/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-1/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-1/checkpoint-3500\n",
      "Configuration saved in prueba_hiperparametros/run-1/checkpoint-3500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-1/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-1/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-1/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-1/checkpoint-2000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 642\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2022-02-17 13:43:56,742]\u001b[0m Trial 1 finished with value: 25.4813 and parameters: {'learning_rate': 0.00016095649628848489, 'num_train_epochs': 2, 'per_device_train_batch_size': 4, 'weight_decay': 0.014805810716977825}. Best is trial 0 with value: 511.0005.\u001b[0m\n",
      "Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 8.4392, 'gen_len': 17.0421}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/config.json from cache at /home/marina/.cache/huggingface/transformers/5f8704e1d92551880c1978068cafdf6503c7a82b65e4494be411d26cb7e86ae5.54793584623797a1e55aa65ed13d20fb17e92502019985cae46974750ad85593\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      33252\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 33252,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 33252,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 33253\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/pytorch_model.bin from cache at /home/marina/.cache/huggingface/transformers/415af22dd09609c5b17fafdc415111b246d167424a6cbf434e09c258728c63b6.eb1842c6d9b4f1ed8611496ed600416387a7972ee684375e1c4640f1691d7e0d\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-es-es.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 7096\n",
      "  Num Epochs = 3\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 5322\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5322' max='5322' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5322/5322 05:05, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.003500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>5.633600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>5.544600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>5.520800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>5.493500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>5.517700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>5.536000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>5.507600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>5.480200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>5.509400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to prueba_hiperparametros/run-2/checkpoint-500\n",
      "Configuration saved in prueba_hiperparametros/run-2/checkpoint-500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-2/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-2/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-2/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to prueba_hiperparametros/run-2/checkpoint-1000\n",
      "Configuration saved in prueba_hiperparametros/run-2/checkpoint-1000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-2/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-2/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-2/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to prueba_hiperparametros/run-2/checkpoint-1500\n",
      "Configuration saved in prueba_hiperparametros/run-2/checkpoint-1500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-2/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-2/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-2/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to prueba_hiperparametros/run-2/checkpoint-2000\n",
      "Configuration saved in prueba_hiperparametros/run-2/checkpoint-2000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-2/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-2/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-2/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-2/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-2/checkpoint-2500\n",
      "Configuration saved in prueba_hiperparametros/run-2/checkpoint-2500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-2/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-2/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-2/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-2/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-2/checkpoint-3000\n",
      "Configuration saved in prueba_hiperparametros/run-2/checkpoint-3000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-2/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-2/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-2/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-2/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-2/checkpoint-3500\n",
      "Configuration saved in prueba_hiperparametros/run-2/checkpoint-3500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-2/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-2/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-2/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-2/checkpoint-2000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-2/checkpoint-4000\n",
      "Configuration saved in prueba_hiperparametros/run-2/checkpoint-4000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-2/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-2/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-2/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-2/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-2/checkpoint-4500\n",
      "Configuration saved in prueba_hiperparametros/run-2/checkpoint-4500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-2/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-2/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-2/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-2/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-2/checkpoint-5000\n",
      "Configuration saved in prueba_hiperparametros/run-2/checkpoint-5000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-2/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-2/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-2/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-2/checkpoint-3500] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 642\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2022-02-17 13:57:07,518]\u001b[0m Trial 2 finished with value: 511.0005 and parameters: {'learning_rate': 0.002763501124891187, 'num_train_epochs': 3, 'per_device_train_batch_size': 4, 'weight_decay': 0.03271572831883526}. Best is trial 0 with value: 511.0005.\u001b[0m\n",
      "Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0005, 'gen_len': 511.0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/config.json from cache at /home/marina/.cache/huggingface/transformers/5f8704e1d92551880c1978068cafdf6503c7a82b65e4494be411d26cb7e86ae5.54793584623797a1e55aa65ed13d20fb17e92502019985cae46974750ad85593\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      33252\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 33252,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 33252,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 33253\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/pytorch_model.bin from cache at /home/marina/.cache/huggingface/transformers/415af22dd09609c5b17fafdc415111b246d167424a6cbf434e09c258728c63b6.eb1842c6d9b4f1ed8611496ed600416387a7972ee684375e1c4640f1691d7e0d\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-es-es.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 7096\n",
      "  Num Epochs = 4\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 3548\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3548' max='3548' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3548/3548 03:45, Epoch 4/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.594700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.671100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.365500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.161900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>1.973600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>1.777300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.682000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to prueba_hiperparametros/run-3/checkpoint-500\n",
      "Configuration saved in prueba_hiperparametros/run-3/checkpoint-500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-3/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-3/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-3/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to prueba_hiperparametros/run-3/checkpoint-1000\n",
      "Configuration saved in prueba_hiperparametros/run-3/checkpoint-1000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-3/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-3/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-3/checkpoint-1000/special_tokens_map.json\n",
      "/home/marina/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to prueba_hiperparametros/run-3/checkpoint-1500\n",
      "Configuration saved in prueba_hiperparametros/run-3/checkpoint-1500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-3/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-3/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-3/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to prueba_hiperparametros/run-3/checkpoint-2000\n",
      "Configuration saved in prueba_hiperparametros/run-3/checkpoint-2000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-3/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-3/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-3/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-3/checkpoint-500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-3/checkpoint-2500\n",
      "Configuration saved in prueba_hiperparametros/run-3/checkpoint-2500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-3/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-3/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-3/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-3/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-3/checkpoint-3000\n",
      "Configuration saved in prueba_hiperparametros/run-3/checkpoint-3000/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-3/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-3/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-3/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-3/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to prueba_hiperparametros/run-3/checkpoint-3500\n",
      "Configuration saved in prueba_hiperparametros/run-3/checkpoint-3500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-3/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-3/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-3/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [prueba_hiperparametros/run-3/checkpoint-2000] due to args.save_total_limit\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 642\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2022-02-17 14:01:19,417]\u001b[0m Trial 3 finished with value: 25.441200000000002 and parameters: {'learning_rate': 0.00016991423248259656, 'num_train_epochs': 4, 'per_device_train_batch_size': 8, 'weight_decay': 0.03244764835061181}. Best is trial 0 with value: 511.0005.\u001b[0m\n",
      "Trial:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 9.0129, 'gen_len': 16.4283}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/config.json from cache at /home/marina/.cache/huggingface/transformers/5f8704e1d92551880c1978068cafdf6503c7a82b65e4494be411d26cb7e86ae5.54793584623797a1e55aa65ed13d20fb17e92502019985cae46974750ad85593\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      33252\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 33252,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 33252,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 33253\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/pytorch_model.bin from cache at /home/marina/.cache/huggingface/transformers/415af22dd09609c5b17fafdc415111b246d167424a6cbf434e09c258728c63b6.eb1842c6d9b4f1ed8611496ed600416387a7972ee684375e1c4640f1691d7e0d\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-es-es.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 7096\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 8\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 8\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 887\n",
      "/home/marina/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='887' max='887' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [887/887 00:54, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>5.905800</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to prueba_hiperparametros/run-4/checkpoint-500\n",
      "Configuration saved in prueba_hiperparametros/run-4/checkpoint-500/config.json\n",
      "Model weights saved in prueba_hiperparametros/run-4/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in prueba_hiperparametros/run-4/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in prueba_hiperparametros/run-4/checkpoint-500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 642\n",
      "  Batch size = 8\n",
      "\u001b[32m[I 2022-02-17 14:10:10,921]\u001b[0m Trial 4 finished with value: 511.0005 and parameters: {'learning_rate': 0.002587833506997122, 'num_train_epochs': 1, 'per_device_train_batch_size': 8, 'weight_decay': 0.029129747516881056}. Best is trial 0 with value: 511.0005.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0005, 'gen_len': 511.0}\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model_init=model_init,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_bleu,\n",
    ")\n",
    "\n",
    "\n",
    "best_trial = trainer.hyperparameter_search(direction=\"maximize\",n_trials=5, hp_space=my_hp_space)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We obtain the best result for the bleu metric in the 4th execution of the method `hyperparameter_search`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "BestRun(run_id='0', objective=511.0005, hyperparameters={'learning_rate': 0.0007469165006449348, 'num_train_epochs': 8, 'per_device_train_batch_size': 4, 'weight_decay': 0.08783717437402438})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_trial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define the arguments to be used by the model during training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "batch_size = 8\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    \"experimentos/full_dataset\",\n",
    "    evaluation_strategy = \"epoch\",\n",
    "    learning_rate=1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    weight_decay=0.03,\n",
    "    save_total_limit=3,\n",
    "    num_train_epochs=8,\n",
    "    predict_with_generate=True,\n",
    "    eval_accumulation_steps=1,\n",
    "    fp16=True,\n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using amp fp16 backend\n"
     ]
    }
   ],
   "source": [
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_bleu,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/config.json from cache at /home/marina/.cache/huggingface/transformers/5f8704e1d92551880c1978068cafdf6503c7a82b65e4494be411d26cb7e86ae5.54793584623797a1e55aa65ed13d20fb17e92502019985cae46974750ad85593\n",
      "Model config MarianConfig {\n",
      "  \"_num_labels\": 3,\n",
      "  \"activation_dropout\": 0.0,\n",
      "  \"activation_function\": \"swish\",\n",
      "  \"add_bias_logits\": false,\n",
      "  \"add_final_layer_norm\": false,\n",
      "  \"architectures\": [\n",
      "    \"MarianMTModel\"\n",
      "  ],\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bad_words_ids\": [\n",
      "    [\n",
      "      33252\n",
      "    ]\n",
      "  ],\n",
      "  \"bos_token_id\": 0,\n",
      "  \"classif_dropout\": 0.0,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_attention_heads\": 8,\n",
      "  \"decoder_ffn_dim\": 2048,\n",
      "  \"decoder_layerdrop\": 0.0,\n",
      "  \"decoder_layers\": 6,\n",
      "  \"decoder_start_token_id\": 33252,\n",
      "  \"dropout\": 0.1,\n",
      "  \"encoder_attention_heads\": 8,\n",
      "  \"encoder_ffn_dim\": 2048,\n",
      "  \"encoder_layerdrop\": 0.0,\n",
      "  \"encoder_layers\": 6,\n",
      "  \"eos_token_id\": 0,\n",
      "  \"forced_eos_token_id\": 0,\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\",\n",
      "    \"1\": \"LABEL_1\",\n",
      "    \"2\": \"LABEL_2\"\n",
      "  },\n",
      "  \"init_std\": 0.02,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0,\n",
      "    \"LABEL_1\": 1,\n",
      "    \"LABEL_2\": 2\n",
      "  },\n",
      "  \"max_length\": 512,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"marian\",\n",
      "  \"normalize_before\": false,\n",
      "  \"normalize_embedding\": false,\n",
      "  \"num_beams\": 4,\n",
      "  \"num_hidden_layers\": 6,\n",
      "  \"pad_token_id\": 33252,\n",
      "  \"scale_embedding\": true,\n",
      "  \"static_position_embeddings\": true,\n",
      "  \"transformers_version\": \"4.9.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 33253\n",
      "}\n",
      "\n",
      "loading weights file https://huggingface.co/Helsinki-NLP/opus-mt-es-es/resolve/main/pytorch_model.bin from cache at /home/marina/.cache/huggingface/transformers/415af22dd09609c5b17fafdc415111b246d167424a6cbf434e09c258728c63b6.eb1842c6d9b4f1ed8611496ed600416387a7972ee684375e1c4640f1691d7e0d\n",
      "All model checkpoint weights were used when initializing MarianMTModel.\n",
      "\n",
      "All the weights of MarianMTModel were initialized from the model checkpoint at Helsinki-NLP/opus-mt-es-es.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use MarianMTModel for predictions without further training.\n",
      "The following columns in the training set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running training *****\n",
      "  Num examples = 7096\n",
      "  Num Epochs = 8\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 14192\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='14192' max='14192' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [14192/14192 19:13, Epoch 8/8]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Bleu</th>\n",
       "      <th>Gen Len</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.761400</td>\n",
       "      <td>2.426999</td>\n",
       "      <td>10.547300</td>\n",
       "      <td>22.825500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.322100</td>\n",
       "      <td>2.167676</td>\n",
       "      <td>11.844400</td>\n",
       "      <td>20.718100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2.034600</td>\n",
       "      <td>2.048408</td>\n",
       "      <td>10.874600</td>\n",
       "      <td>16.049800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>1.767500</td>\n",
       "      <td>2.014832</td>\n",
       "      <td>12.667600</td>\n",
       "      <td>18.940800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>1.525500</td>\n",
       "      <td>2.023945</td>\n",
       "      <td>12.576800</td>\n",
       "      <td>17.420600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>1.320800</td>\n",
       "      <td>2.048484</td>\n",
       "      <td>12.046400</td>\n",
       "      <td>16.852000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>1.149900</td>\n",
       "      <td>2.077922</td>\n",
       "      <td>11.713100</td>\n",
       "      <td>16.523400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>1.009500</td>\n",
       "      <td>2.103830</td>\n",
       "      <td>11.857300</td>\n",
       "      <td>16.457900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-1000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-1000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-1500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-1500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-1500/special_tokens_map.json\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 642\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 10.5473, 'gen_len': 22.8255}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-2000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-2000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-2000/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-500] due to args.save_total_limit\n",
      "/home/marina/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-2500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-2500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-2500/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-1000] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-3000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-3000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-3000/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-1500] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-3500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-3500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-3500/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-2000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 642\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 11.8444, 'gen_len': 20.7181}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-4000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-4000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-4000/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-2500] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-4500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-4500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-4500/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-3000] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-5000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-5000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-5000/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-3500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 642\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 10.8746, 'gen_len': 16.0498}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-5500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-5500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-5500/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-4000] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-6000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-6000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-6000/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-4500] due to args.save_total_limit\n",
      "/home/marina/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-6500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-6500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-6500/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-5000] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-7000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-7000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-7000/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-5500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 642\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 12.6676, 'gen_len': 18.9408}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-7500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-7500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-7500/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-6000] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-8000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-8000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-8000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-8000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-8000/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-6500] due to args.save_total_limit\n",
      "/home/marina/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-8500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-8500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-8500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-8500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-8500/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-7000] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 642\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 12.5768, 'gen_len': 17.4206}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-9000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-9000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-9000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-9000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-9000/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-7500] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-9500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-9500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-9500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-9500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-9500/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-8000] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-10000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-10000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-10000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-10000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-10000/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-8500] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-10500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-10500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-10500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-10500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-10500/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-9000] due to args.save_total_limit\n",
      "/home/marina/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 642\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 12.0464, 'gen_len': 16.852}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-11000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-11000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-11000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-11000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-11000/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-9500] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-11500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-11500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-11500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-11500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-11500/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-10000] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-12000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-12000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-12000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-12000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-12000/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-10500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 642\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 11.7131, 'gen_len': 16.5234}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-12500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-12500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-12500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-12500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-12500/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-11000] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-13000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-13000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-13000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-13000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-13000/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-11500] due to args.save_total_limit\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-13500\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-13500/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-13500/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-13500/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-13500/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-12000] due to args.save_total_limit\n",
      "/home/marina/anaconda3/lib/python3.8/site-packages/transformers/trainer.py:1314: FutureWarning: Non-finite norm encountered in torch.nn.utils.clip_grad_norm_; continuing anyway. Note that the default behavior will change in a future release to error out if a non-finite total norm is encountered. At that point, setting error_if_nonfinite=false will be required to retain the old behavior.\n",
      "  nn.utils.clip_grad_norm_(\n",
      "Saving model checkpoint to experimentos/full_dataset/checkpoint-14000\n",
      "Configuration saved in experimentos/full_dataset/checkpoint-14000/config.json\n",
      "Model weights saved in experimentos/full_dataset/checkpoint-14000/pytorch_model.bin\n",
      "tokenizer config file saved in experimentos/full_dataset/checkpoint-14000/tokenizer_config.json\n",
      "Special tokens file saved in experimentos/full_dataset/checkpoint-14000/special_tokens_map.json\n",
      "Deleting older checkpoint [experimentos/full_dataset/checkpoint-12500] due to args.save_total_limit\n",
      "The following columns in the evaluation set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 642\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 11.8573, 'gen_len': 16.4579}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trained = trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the predictions on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The following columns in the test set  don't have a corresponding argument in `MarianMTModel.forward` and have been ignored: translation.\n",
      "***** Running Prediction *****\n",
      "  Num examples = 642\n",
      "  Batch size = 4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 11.8573, 'gen_len': 16.4579}\n",
      "{'eval_loss': 2.103830099105835, 'eval_bleu': 11.8573, 'eval_gen_len': 16.4579, 'eval_runtime': 33.9896, 'eval_samples_per_second': 18.888, 'eval_steps_per_second': 4.737}\n"
     ]
    }
   ],
   "source": [
    "predictions = trainer.predict(tokenized_datasets[\"test\"])\n",
    "print(predictions.metrics)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
